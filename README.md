Зважаючи на те, що відновлена моделі не змогла навчитись на даних, як це було описано у статті, є велика ймовірність, що проблема закралась на етапі попередньої обробки даних. Також є кілька варіантів, як можна спробувати покращити результати експериментуючи із різними параметрами самої моделі. Нижче наведено п’ять можливих шляхів оптимізації наявної моделі.

1.	Виведення попередньої обробки даних зі структури нейронної мережі і проведення мануальної обробки. Перевірка правильності обробки вручну може виявити помилки при написанні коду, які були не помітні, коли обробка автоматично вшивалась у модель.

2.	Embedding Layer Wieghts: Було б добре поекспериментувати з можливістю навчання Embedding шару. Навчання цього шару (trainable=True) дозволяє моделі тонко налаштовувати їх під час навчання. Варто прослідкувати за потенційним переналаштуванням і оцінити, чи дає навчання Embedding шару переваги.

3.	Batch Normalization: Можна поекспериментувати з увімкненням/вимкненням Batch Normalization після кожного шару згортки. Batch Normalization може мати різний вплив, тому варто спостерігати за її впливом на стабільність навчання та продуктивність моделі.

4.	Dropout: Варто спробувати різні коефіцієнти Dropout в Dense шарах, наприклад, 0,2 або 0,3, може бути гарною ідеєю. Регулювання частоти Dropout дозволить систематично дослідити її вплив на запобігання надмірному оверфіту.

5.	Розмір Dense Layer: Варто поекспериментувати з різними розмірами Dense шару перед створенням вихідного шару. Це дозволить оцінити, як розмір впливає на здатність моделі вловлювати відповідні особливості.
